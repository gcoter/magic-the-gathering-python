{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from abc import abstractmethod\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state_global_dim = 2\n",
    "n_players = 2\n",
    "player_dim = 8\n",
    "action_general_dim = 31\n",
    "zone_vector_dim = 34\n",
    "\n",
    "max_n_zone_vectors = 120\n",
    "max_n_action_source_cards = 10\n",
    "max_n_action_target_cards = 10\n",
    "\n",
    "embedding_dim = 64\n",
    "transformer_n_layers = 5\n",
    "transformer_n_heads = 16\n",
    "transformer_dim_feedforward = 128\n",
    "dropout = 0.0\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 10\n",
    "early_stopping_patience = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "mlflow.log_param(\"transformer_n_layers\", transformer_n_layers)\n",
    "mlflow.log_param(\"transformer_n_heads\", transformer_n_heads)\n",
    "mlflow.log_param(\"transformer_dim_feedforward\", transformer_dim_feedforward)\n",
    "mlflow.log_param(\"dropout\", dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read one pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_logs_folder_path = \"../data/game_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_from_pickles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(game_logs_folder_path):\n",
    "    game_log_file_path = os.path.join(game_logs_folder_path, file_name)\n",
    "    print(f\"Read game logs from '{game_log_file_path}'\")\n",
    "    with open(game_log_file_path, \"rb\") as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "        for item_dict in data_dict[\"dataset\"]:\n",
    "            n_possible_actions = len(item_dict[\"possible_actions\"])\n",
    "            if n_possible_actions >= 2:\n",
    "                dataset_from_pickles.append(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_from_pickles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(\n",
    "    vec: torch.Tensor,\n",
    "    pad: int,\n",
    "    dim: int,\n",
    "    device,\n",
    "    return_pad_size: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = 0\n",
    "    if pad > vec.size(dim):\n",
    "        pad_shape = list(vec.shape)\n",
    "        pad_size = pad - vec.size(dim)\n",
    "        pad_shape[dim] = pad_size\n",
    "        padded_tensor = torch.cat([vec.to(device), torch.zeros(*pad_shape).to(device)], dim=dim)\n",
    "    else:\n",
    "        padded_tensor = torch.from_numpy(vec.cpu().numpy().take(torch.arange(pad), axis=dim)).to(device)\n",
    "    if return_pad_size:\n",
    "        return padded_tensor, pad_size\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        player_dataset: List[Dict],\n",
    "        zone_vector_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.player_dataset = player_dataset\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.player_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        item_dict = self.player_dataset[idx]\n",
    "        action_history = item_dict[\"action_history\"]\n",
    "        current_game_state = item_dict[\"current_game_state\"]\n",
    "        possible_actions = item_dict[\"possible_actions\"]\n",
    "        chosen_action_index = item_dict[\"chosen_action_index\"]\n",
    "\n",
    "        action_history_vectors = self.__get_action_history_vectors(action_history)\n",
    "        current_game_state_vectors = self.__get_current_game_state_vectors(current_game_state)\n",
    "        possible_actions_vectors = self.__get_possible_actions_vectors(possible_actions)\n",
    "        target_action = self.__get_target_action(\n",
    "            n_possible_actions=len(possible_actions),\n",
    "            chosen_action_index=chosen_action_index\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_history_vectors,\n",
    "            current_game_state_vectors,\n",
    "            possible_actions_vectors,\n",
    "            target_action\n",
    "        )\n",
    "\n",
    "    def __action_list_to_tensors(self, action_list: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_list:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_list_vectors\n",
    "        - general: (n_actions, action_dim)\n",
    "        - source_card_vectors: (n_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        general_vectors = []\n",
    "        source_card_vectors = []\n",
    "        target_card_vectors = []\n",
    "\n",
    "        for action_dict in action_list:\n",
    "            general = torch.from_numpy(action_dict[\"general\"]).to(self.device)\n",
    "            source_cards = torch.from_numpy(action_dict[\"source_card_vectors\"])\n",
    "            if len(source_cards) == 0:\n",
    "                source_cards = torch.zeros(size=(self.max_n_action_source_cards, self.zone_vector_dim)).to(self.device)\n",
    "            else:\n",
    "                source_cards = pad_tensor(\n",
    "                    source_cards,\n",
    "                    pad=self.max_n_action_source_cards,\n",
    "                    dim=0,\n",
    "                    device=self.device\n",
    "                ).to(self.device)\n",
    "\n",
    "            target_cards = torch.from_numpy(action_dict[\"target_card_vectors\"])\n",
    "            if len(target_cards) == 0:\n",
    "                target_cards = torch.zeros(size=(self.max_n_action_target_cards, self.zone_vector_dim)).to(self.device)\n",
    "            else:\n",
    "                target_cards = pad_tensor(\n",
    "                    target_cards,\n",
    "                    pad=self.max_n_action_target_cards,\n",
    "                    dim=0,\n",
    "                    device=self.device\n",
    "                ).to(self.device)\n",
    "\n",
    "            general_vectors.append(general[None])\n",
    "            source_card_vectors.append(source_cards[None])\n",
    "            target_card_vectors.append(target_cards[None])\n",
    "\n",
    "        return {\n",
    "            \"general\": torch.cat(general_vectors, dim=0).float().to(self.device),\n",
    "            \"source_card_vectors\": torch.cat(source_card_vectors, dim=0).float().to(self.device),\n",
    "            \"target_card_vectors\": torch.cat(target_card_vectors, dim=0).float().to(self.device),\n",
    "        }\n",
    "\n",
    "    def __get_action_history_vectors(self, action_history: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_history:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * history_size\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_history_vectors\n",
    "        - general: (history_size, action_dim)\n",
    "        - source_card_vectors: (history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=action_history)\n",
    "\n",
    "    def __get_current_game_state_vectors(self, current_game_state: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        current_game_state:\n",
    "        {\n",
    "            global: (global_dim,)\n",
    "            players: (n_players, player_dim)\n",
    "            zones: (n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Return:\n",
    "\n",
    "        current_game_state_vectors:\n",
    "        - global: (global_dim,)\n",
    "        - players: (n_players, player_dim)\n",
    "        - zones: (max_n_zone_vectors, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"global\": torch.from_numpy(current_game_state[\"global\"]).float().to(self.device),\n",
    "            \"players\": torch.from_numpy(current_game_state[\"players\"]).float().to(self.device),\n",
    "            \"zones\": pad_tensor(\n",
    "                torch.from_numpy(current_game_state[\"zones\"]),\n",
    "                pad=self.max_n_zone_vectors,\n",
    "                dim=0,\n",
    "                device=self.device\n",
    "            ).float().to(self.device)\n",
    "        }\n",
    "\n",
    "    def __get_possible_actions_vectors(self, possible_actions: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        possible_actions:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_possible_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        possible_actions_vectors:\n",
    "        - general: (n_possible_actions, action_dim)\n",
    "        - source_card_vectors: (n_possible_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_possible_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=possible_actions)\n",
    "\n",
    "    def __get_target_action(self, n_possible_actions: int, chosen_action_index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return:\n",
    "\n",
    "        target_action: (n_possible_actions,)\n",
    "        \"\"\"\n",
    "        target_action = torch.zeros(n_possible_actions).float().to(self.device)\n",
    "        target_action[chosen_action_index] = 1\n",
    "        return target_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_dataset = DeepLearningDataset(\n",
    "    player_dataset=dataset_from_pickles,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_possible_actions_collate_fn(\n",
    "    samples: List[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]],\n",
    "    device\n",
    ") -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "    batch_action_history_vectors = {}\n",
    "    batch_current_game_state_vectors = {}\n",
    "    batch_possible_actions_vectors = {}\n",
    "    batch_target_action = []\n",
    "    batch_n_possible_actions = []\n",
    "\n",
    "    max_n_possible_actions = max([sample[2][\"general\"].shape[0] for sample in samples])\n",
    "\n",
    "    for sample in samples:\n",
    "        for key, tensor in sample[0].items():\n",
    "            if key not in batch_action_history_vectors:\n",
    "                batch_action_history_vectors[key] = []\n",
    "            batch_action_history_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[1].items():\n",
    "            if key not in batch_current_game_state_vectors:\n",
    "                batch_current_game_state_vectors[key] = []\n",
    "            batch_current_game_state_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[2].items():\n",
    "            if key not in batch_possible_actions_vectors:\n",
    "                batch_possible_actions_vectors[key] = []\n",
    "            tensor = pad_tensor(tensor, pad=max_n_possible_actions, dim=0, device=device)\n",
    "            batch_possible_actions_vectors[key].append(tensor[None])\n",
    "\n",
    "        padded_target_action = pad_tensor(\n",
    "            sample[3],\n",
    "            pad=max_n_possible_actions,\n",
    "            dim=0,\n",
    "            device=device\n",
    "        )\n",
    "        batch_target_action.append(padded_target_action[None])\n",
    "        batch_n_possible_actions.append(int(sample[3].shape[0]))\n",
    "\n",
    "    for key, tensors in batch_action_history_vectors.items():\n",
    "        batch_action_history_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    for key, tensors in batch_current_game_state_vectors.items():\n",
    "        batch_current_game_state_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    for key, tensors in batch_possible_actions_vectors.items():\n",
    "        batch_possible_actions_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    batch_target_action = torch.cat(batch_target_action, dim=0).to(device)\n",
    "    batch_n_possible_actions = torch.from_numpy(np.array(batch_n_possible_actions)).to(device)\n",
    "    \n",
    "    return (\n",
    "        batch_action_history_vectors,\n",
    "        batch_current_game_state_vectors,\n",
    "        batch_possible_actions_vectors,\n",
    "        batch_target_action,\n",
    "        batch_n_possible_actions\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepLearningScorer(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        batch_action_history_vectors:\n",
    "        {\n",
    "            general: (batch_size, history_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_current_game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_possible_actions_vectors:\n",
    "        {\n",
    "            general: (batch_size, max_n_possible_actions_in_batch, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Returns:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __step(self, batch, batch_idx, base_metric_name):\n",
    "        batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors, batch_target_action, batch_n_possible_actions = batch\n",
    "        batch_predicted_target_action = self.forward(\n",
    "            batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors\n",
    "        )\n",
    "\n",
    "        batch_loss = self.loss(batch_predicted_target_action, batch_target_action)\n",
    "\n",
    "        \"\"\"\n",
    "        batch_loss = 0.0\n",
    "        for predicted_target_action, n_possible_actions, target_action in zip(batch_predicted_target_action, batch_n_possible_actions, batch_target_action):\n",
    "            batch_loss += self.loss(predicted_target_action[:n_possible_actions], target_action[:n_possible_actions]) / n_possible_actions\n",
    "        batch_loss /= len(batch_predicted_target_action)\n",
    "        \"\"\"\n",
    "\n",
    "        self.log(f\"{base_metric_name}_loss\", batch_loss, on_epoch=True, prog_bar=True)\n",
    "        return batch_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"training\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"validation\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        batch_game_state_vectors, batch_action_vectors = batch\n",
    "        batch_predicted_scores = self.forward(\n",
    "            batch_game_state_vectors=batch_game_state_vectors, batch_action_vectors=batch_action_vectors\n",
    "        )\n",
    "        return batch_predicted_scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def get_n_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder class for Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 dropout,\n",
    "                 bidir):\n",
    "        \"\"\"\n",
    "        Initiate Encoder\n",
    "\n",
    "        :param Tensor embedding_dim: Number of embbeding channels\n",
    "        :param int hidden_dim: Number of hidden units for the LSTM\n",
    "        :param int n_layers: Number of layers for LSTMs\n",
    "        :param float dropout: Float between 0-1\n",
    "        :param bool bidir: Bidirectional\n",
    "        \"\"\"\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim//2 if bidir else hidden_dim\n",
    "        self.n_layers = n_layers*2 if bidir else n_layers\n",
    "        self.bidir = bidir\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            self.hidden_dim,\n",
    "                            n_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidir)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.h0 = Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.c0 = Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, embedded_inputs,\n",
    "                hidden):\n",
    "        \"\"\"\n",
    "        Encoder - Forward-pass\n",
    "\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net\n",
    "        :param Tensor hidden: Initiated hidden units for the LSTMs (h, c)\n",
    "        :return: LSTMs outputs and hidden units (h, c)\n",
    "        \"\"\"\n",
    "\n",
    "        embedded_inputs = embedded_inputs.permute(1, 0, 2)\n",
    "\n",
    "        outputs, hidden = self.lstm(embedded_inputs, hidden)\n",
    "\n",
    "        return outputs.permute(1, 0, 2), hidden\n",
    "\n",
    "    def init_hidden(self, embedded_inputs):\n",
    "        \"\"\"\n",
    "        Initiate hidden units\n",
    "\n",
    "        :param Tensor embedded_inputs: The embedded input of Pointer-NEt\n",
    "        :return: Initiated hidden units for the LSTMs (h, c)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "\n",
    "        # Reshaping (Expanding)\n",
    "        h0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers,\n",
    "                                                      batch_size,\n",
    "                                                      self.hidden_dim)\n",
    "        c0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers,\n",
    "                                                      batch_size,\n",
    "                                                      self.hidden_dim)\n",
    "\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention model for Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Attention\n",
    "\n",
    "        :param int input_dim: Input's diamention\n",
    "        :param int hidden_dim: Number of hidden units in the attention\n",
    "        \"\"\"\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.context_linear = nn.Conv1d(input_dim, hidden_dim, 1, 1)\n",
    "        self.V = Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "        self._inf = Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # Initialize vector V\n",
    "        nn.init.uniform(self.V, -1, 1)\n",
    "\n",
    "    def forward(self, input,\n",
    "                context,\n",
    "                mask):\n",
    "        \"\"\"\n",
    "        Attention - Forward-pass\n",
    "\n",
    "        :param Tensor input: Hidden state h\n",
    "        :param Tensor context: Attention context\n",
    "        :param ByteTensor mask: Selection mask\n",
    "        :return: tuple of - (Attentioned hidden state, Alphas)\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch, hidden_dim, seq_len)\n",
    "        inp = self.input_linear(input).unsqueeze(2).expand(-1, -1, context.size(1))\n",
    "\n",
    "        # (batch, hidden_dim, seq_len)\n",
    "        context = context.permute(0, 2, 1)\n",
    "        ctx = self.context_linear(context)\n",
    "\n",
    "        # (batch, 1, hidden_dim)\n",
    "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
    "        if len(att[mask]) > 0:\n",
    "            att[mask] = self.inf[mask]\n",
    "        alpha = self.softmax(att)\n",
    "\n",
    "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return hidden_state, alpha\n",
    "\n",
    "    def init_inf(self, mask_size):\n",
    "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder model for Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Decoder\n",
    "\n",
    "        :param int embedding_dim: Number of embeddings in Pointer-Net\n",
    "        :param int hidden_dim: Number of hidden units for the decoder's RNN\n",
    "        \"\"\"\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.att = Attention(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.mask = Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.runner = Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, embedded_inputs,\n",
    "                decoder_input,\n",
    "                hidden,\n",
    "                context):\n",
    "        \"\"\"\n",
    "        Decoder - Forward-pass\n",
    "\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net\n",
    "        :param Tensor decoder_input: First decoder's input\n",
    "        :param Tensor hidden: First decoder's hidden states\n",
    "        :param Tensor context: Encoder's outputs\n",
    "        :return: (Output probabilities, Pointers indices), last hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "        input_length = embedded_inputs.size(1)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "        self.att.init_inf(mask.size())\n",
    "\n",
    "        # Generating arang(input_length), broadcasted across batch_size\n",
    "        runner = self.runner.repeat(input_length)\n",
    "        for i in range(input_length):\n",
    "            runner.data[i] = i\n",
    "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "\n",
    "        outputs = []\n",
    "        pointers = []\n",
    "\n",
    "        def step(x, hidden):\n",
    "            \"\"\"\n",
    "            Recurrence step function\n",
    "\n",
    "            :param Tensor x: Input at time t\n",
    "            :param tuple(Tensor, Tensor) hidden: Hidden states at time t-1\n",
    "            :return: Hidden states at time t (h, c), Attention probabilities (Alpha)\n",
    "            \"\"\"\n",
    "\n",
    "            # Regular LSTM\n",
    "            h, c = hidden\n",
    "\n",
    "            gates = self.input_to_hidden(x) + self.hidden_to_hidden(h)\n",
    "            input, forget, cell, out = gates.chunk(4, 1)\n",
    "\n",
    "            input = F.sigmoid(input)\n",
    "            forget = F.sigmoid(forget)\n",
    "            cell = F.tanh(cell)\n",
    "            out = F.sigmoid(out)\n",
    "\n",
    "            c_t = (forget * c) + (input * cell)\n",
    "            h_t = out * F.tanh(c_t)\n",
    "\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = F.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "\n",
    "            return hidden_t, c_t, output\n",
    "\n",
    "        # Recurrence loop\n",
    "        for _ in range(input_length):\n",
    "            h_t, c_t, outs = step(decoder_input, hidden)\n",
    "            hidden = (h_t, c_t)\n",
    "\n",
    "            # Masking selected inputs\n",
    "            masked_outs = outs * mask\n",
    "\n",
    "            # Get maximum probabilities and indices\n",
    "            max_probs, indices = masked_outs.max(1)\n",
    "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
    "\n",
    "            # Update mask to ignore seen indices\n",
    "            mask  = mask * (1 - one_hot_pointers)\n",
    "\n",
    "            # Get embedded inputs by max indices\n",
    "            embedding_mask = one_hot_pointers.unsqueeze(2).expand(-1, -1, self.embedding_dim).byte()\n",
    "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
    "\n",
    "            outputs.append(outs.unsqueeze(0))\n",
    "            pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "        pointers = torch.cat(pointers, 1)\n",
    "\n",
    "        return (outputs, pointers), hidden\n",
    "\n",
    "\n",
    "class PointerNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 lstm_layers,\n",
    "                 dropout,\n",
    "                 bidir=False):\n",
    "        \"\"\"\n",
    "        Initiate Pointer-Net\n",
    "\n",
    "        :param int embedding_dim: Number of embbeding channels\n",
    "        :param int hidden_dim: Encoders hidden units\n",
    "        :param int lstm_layers: Number of layers for LSTMs\n",
    "        :param float dropout: Float between 0-1\n",
    "        :param bool bidir: Bidirectional\n",
    "        \"\"\"\n",
    "\n",
    "        super(PointerNet, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.bidir = bidir\n",
    "        self.encoder = Encoder(embedding_dim,\n",
    "                               hidden_dim,\n",
    "                               lstm_layers,\n",
    "                               dropout,\n",
    "                               bidir)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_dim)\n",
    "        self.decoder_input0 = Parameter(torch.FloatTensor(embedding_dim), requires_grad=False)\n",
    "\n",
    "        # Initialize decoder_input0\n",
    "        nn.init.uniform(self.decoder_input0, -1, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        PointerNet - Forward-pass\n",
    "\n",
    "        :param Tensor inputs: Input sequence\n",
    "        :return: Pointers probabilities and indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "\n",
    "        decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        embedded_inputs = inputs\n",
    "\n",
    "        encoder_hidden0 = self.encoder.init_hidden(embedded_inputs)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(embedded_inputs,\n",
    "                                                       encoder_hidden0)\n",
    "        if self.bidir:\n",
    "            decoder_hidden0 = (torch.cat(encoder_hidden[0][-2:], dim=-1),\n",
    "                               torch.cat(encoder_hidden[1][-2:], dim=-1))\n",
    "        else:\n",
    "            decoder_hidden0 = (encoder_hidden[0][-1],\n",
    "                               encoder_hidden[1][-1])\n",
    "        (outputs, pointers), decoder_hidden = self.decoder(embedded_inputs,\n",
    "                                                           decoder_input0,\n",
    "                                                           decoder_hidden0,\n",
    "                                                           encoder_outputs)\n",
    "\n",
    "        return  outputs, pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProcessingBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.general_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.action_general_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.card_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, action_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - action_vectors:\n",
    "        {\n",
    "            general: (batch_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - action_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = action_vectors[\"general\"].shape[0]\n",
    "\n",
    "        action_general_embedding = self.__get_action_general_embedding(action_vectors[\"general\"])\n",
    "        action_source_card_embeddings = self.__get_action_source_card_embeddings(action_vectors[\"source_card_vectors\"])\n",
    "        action_target_card_embeddings = self.__get_action_target_card_embeddings(action_vectors[\"target_card_vectors\"])\n",
    "\n",
    "        action_embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim).to(action_general_embedding)\n",
    "\n",
    "        action_embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                action_embedding_for_prediction,\n",
    "                action_general_embedding[:, None],\n",
    "                action_source_card_embeddings,\n",
    "                action_target_card_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        action_embeddings_sequence_after_transformer = self.transformer_encoder(action_embeddings_sequence)\n",
    "\n",
    "        return action_embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_action_general_embedding(self, action_general_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_general_vector.shape[0]\n",
    "        action_general_embedding = self.general_mlp(action_general_vector)\n",
    "        action_general_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1).to(action_general_embedding)\n",
    "        return torch.cat([action_general_embedding, action_general_embedding_type], dim=1)\n",
    "\n",
    "    def __get_action_source_card_embeddings(self, action_source_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_source_card_vectors.shape[0]\n",
    "        action_source_card_embeddings = self.card_mlp(action_source_card_vectors)\n",
    "        action_source_card_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.max_n_action_source_cards, 1).to(action_source_card_embeddings)\n",
    "        return torch.cat([action_source_card_embeddings, action_source_card_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_action_target_card_embeddings(self, action_target_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_target_card_vectors.shape[0]\n",
    "        action_target_card_embeddings = self.card_mlp(action_target_card_vectors)\n",
    "        action_target_card_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_action_target_cards, 1).to(action_target_card_embeddings)\n",
    "        return torch.cat([action_target_card_embeddings, action_target_card_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class GameStateProcessingBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.global_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.game_state_global_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.player_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.player_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.zone_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, game_state_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - game_state_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = game_state_vectors[\"global\"].shape[0]\n",
    "\n",
    "        global_embedding = self.__get_global_embedding(game_state_vectors[\"global\"])\n",
    "        player_embeddings = self.__get_player_embeddings(game_state_vectors[\"players\"])\n",
    "        zone_embeddings = self.__get_zone_embeddings(game_state_vectors[\"zones\"])\n",
    "\n",
    "        embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim).to(global_embedding)\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                embedding_for_prediction,\n",
    "                global_embedding[:, None],\n",
    "                player_embeddings,\n",
    "                zone_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        return embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_global_embedding(self, game_state_global_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_global_vector.shape[0]\n",
    "        global_embedding = self.global_mlp(game_state_global_vector)\n",
    "        global_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1).to(global_embedding)\n",
    "        return torch.cat([global_embedding, global_embedding_type], dim=1)\n",
    "\n",
    "    def __get_player_embeddings(self, game_state_player_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_player_vectors.shape[0]\n",
    "        player_embeddings = self.player_mlp(game_state_player_vectors)\n",
    "        player_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.n_players, 1).to(player_embeddings)\n",
    "        return torch.cat([player_embeddings, player_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_zone_embeddings(self, game_state_zone_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_zone_vectors.shape[0]\n",
    "        zone_embeddings = self.zone_mlp(game_state_zone_vectors)\n",
    "        zone_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_zone_vectors, 1).to(zone_embeddings)\n",
    "        return torch.cat([zone_embeddings, zone_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class ClassificationBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        pointer_net_n_lstm_layers: int = 1,\n",
    "        pointer_net_hidden_dim: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.pointer_net_n_lstm_layers = pointer_net_n_lstm_layers\n",
    "        self.pointer_net_hidden_dim = pointer_net_hidden_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.preprocessing_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_dim + 3, out_features=self.input_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.input_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "        self.pointer_net = PointerNet(\n",
    "            embedding_dim=self.input_dim,\n",
    "            hidden_dim=self.pointer_net_hidden_dim,\n",
    "            lstm_layers=self.pointer_net_n_lstm_layers,\n",
    "            dropout=self.dropout,\n",
    "            bidir=False\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_action_history_embeddings: torch.Tensor,\n",
    "        batch_current_game_state_embedding: torch.Tensor,\n",
    "        batch_possible_actions_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - batch_action_history_embeddings: (batch_size, history_size, input_dim)\n",
    "        - batch_current_game_state_embedding: (batch_size, input_dim)\n",
    "        - batch_possible_actions_embeddings: (batch_size, max_n_possible_actions_in_batch, input_dim)\n",
    "\n",
    "        Outputs:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "\n",
    "        batch_action_history_embeddings = self.__prepare_action_history_embeddings(\n",
    "            batch_action_history_embeddings\n",
    "        )\n",
    "        batch_current_game_state_embedding = self.__prepare_current_game_state_embedding(\n",
    "            batch_current_game_state_embedding\n",
    "        )\n",
    "        batch_possible_actions_embeddings = self.__prepare_possible_actions_embeddings(\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                batch_action_history_embeddings,\n",
    "                batch_current_game_state_embedding,\n",
    "                batch_possible_actions_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        possible_actions_embeddings = embeddings_sequence_after_transformer[:, -max_n_possible_actions_in_batch:]\n",
    "\n",
    "        predicted_target_action_probabilities, predicted_target_action_pointers = self.pointer_net(possible_actions_embeddings)\n",
    "\n",
    "        return predicted_target_action_probabilities[..., 0]\n",
    "\n",
    "    def __prepare_action_history_embeddings(self, batch_action_history_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_action_history_embeddings.shape[0]\n",
    "        history_size = batch_action_history_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[1.0, 0.0, 0.0]]]).repeat(batch_size, history_size, 1).to(batch_action_history_embeddings)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_action_history_embeddings, embeddings_type], dim=2))\n",
    "\n",
    "    def __prepare_current_game_state_embedding(self, batch_current_game_state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_current_game_state_embedding.shape[0]\n",
    "        embedding_type = torch.tensor([[0.0, 1.0, 0.0]]).repeat(batch_size, 1).to(batch_current_game_state_embedding)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_current_game_state_embedding, embedding_type], dim=1)[:, None])\n",
    "\n",
    "    def __prepare_possible_actions_embeddings(self, batch_possible_actions_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_possible_actions_embeddings.shape[0]\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, max_n_possible_actions_in_batch, 1).to(batch_possible_actions_embeddings)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_possible_actions_embeddings, embeddings_type], dim=2))\n",
    "\n",
    "\n",
    "class DeepLearningScorerV1(BaseDeepLearningScorer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_n_layers: int,\n",
    "        transformer_n_heads: int,\n",
    "        transformer_dim_feedforward: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.action_processing_block = ActionProcessingBlock(\n",
    "            action_general_dim=self.action_general_dim,\n",
    "            max_n_action_source_cards=self.max_n_action_source_cards,\n",
    "            max_n_action_target_cards=self.max_n_action_target_cards,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=1,\n",
    "            transformer_dim_feedforward=128,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.game_state_processing_block = GameStateProcessingBlock(\n",
    "            game_state_global_dim=self.game_state_global_dim,\n",
    "            n_players=self.n_players,\n",
    "            player_dim=self.player_dim,\n",
    "            max_n_zone_vectors=self.max_n_zone_vectors,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=self.transformer_n_heads,\n",
    "            transformer_dim_feedforward=self.transformer_dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.classification_block = ClassificationBlock(\n",
    "            input_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=self.transformer_n_heads,\n",
    "            transformer_dim_feedforward=self.transformer_dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        batch_action_history_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_action_history_vectors,\n",
    "            n_actions=batch_action_history_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_current_game_state_embedding = self.game_state_processing_block(batch_current_game_state_vectors)\n",
    "\n",
    "        batch_possible_actions_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_possible_actions_vectors,\n",
    "            n_actions=batch_possible_actions_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_predicted_target_action = self.classification_block(\n",
    "            batch_action_history_embeddings,\n",
    "            batch_current_game_state_embedding,\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        return batch_predicted_target_action\n",
    "\n",
    "    def __process_action_list(self, batch_action_list_vectors: torch.Tensor, n_actions: int) -> torch.Tensor:\n",
    "        action_embeddings = []\n",
    "        for i in range(n_actions):\n",
    "            one_action_vectors = {key: tensor[:, i] for key, tensor in batch_action_list_vectors.items()}\n",
    "            action_embedding = self.action_processing_block(one_action_vectors)\n",
    "            action_embeddings.append(action_embedding[:, None])\n",
    "        return torch.cat(action_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLearningScorerV1(\n",
    "    game_state_global_dim=game_state_global_dim,\n",
    "    n_players=n_players,\n",
    "    player_dim=player_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    action_general_dim=action_general_dim,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    embedding_dim=embedding_dim,\n",
    "    transformer_n_layers=transformer_n_layers,\n",
    "    transformer_n_heads=transformer_n_heads,\n",
    "    transformer_dim_feedforward=transformer_dim_feedforward,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    deep_learning_dataset,\n",
    "    lengths=[\n",
    "        0.8,\n",
    "        0.2,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda samples: pad_possible_actions_collate_fn(samples=samples, device=device)\n",
    ")\n",
    "validation_data_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda samples: pad_possible_actions_collate_fn(samples=samples, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_folder_path = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path(model_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "callbacks = [\n",
    "    # ModelCheckpoint(\n",
    "    #     dirpath=model_folder_path,\n",
    "    #     filename=\"deep_learning_scorer\",\n",
    "    #     monitor=\"validation_loss\",\n",
    "    #     mode=\"min\",\n",
    "    #     save_top_k=1,\n",
    "    #     verbose=False\n",
    "    # ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"validation_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=early_stopping_patience,\n",
    "        verbose=False\n",
    "    ),\n",
    "]\n",
    "trainer = Trainer(\n",
    "    max_epochs=n_epochs,\n",
    "    devices=\"auto\",\n",
    "    deterministic=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=training_data_loader,\n",
    "    val_dataloaders=validation_data_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
