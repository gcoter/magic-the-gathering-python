{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from abc import abstractmethod\n",
    "from pytorch_lightning import LightningModule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read one pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_log_file_path = \"../data/game_logs/game_9f9f5c4b-82a6-4051-8bb7-96b3431d569e.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(game_log_file_path, \"rb\") as f:\n",
    "    data_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['game_id', 'dataset', 'winner_player_index'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_id = data_dict[\"game_id\"]\n",
    "dataset = data_dict[\"dataset\"]\n",
    "winner_player_index = data_dict[\"winner_player_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9f9f5c4b-82a6-4051-8bb7-96b3431d569e'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winner_player_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_dict = dataset[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['action_history', 'current_game_state', 'possible_actions', 'chosen_action_index'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = item_dict[\"action_history\"]\n",
    "game_state_vectors = item_dict[\"current_game_state\"]\n",
    "possible_actions = item_dict[\"possible_actions\"]\n",
    "chosen_action_index = item_dict[\"chosen_action_index\"]\n",
    "source_player_index = possible_actions[chosen_action_index][\"source_player_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state_global_dim = game_state_vectors[\"global\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_state_global_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_players = game_state_vectors[\"players\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_dim = game_state_vectors[\"players\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "player_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_general_dim = action_list[0][\"general\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_general_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_vector_dim = game_state_vectors[\"zones\"].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_player_index': 0,\n",
       " 'general': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'source_card_vectors': array([], dtype=float32),\n",
       " 'target_card_vectors': array([[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.]], dtype=float32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'global': array([9., 1.], dtype=float32),\n",
       " 'players': array([[19.,  0.,  0.,  0.,  0.,  0.,  1.,  1.],\n",
       "        [20.,  0.,  0.,  0.,  0.,  0.,  1.,  0.]], dtype=float32),\n",
       " 'zones': array([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 7., 1., 0.,\n",
       "         0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 1.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 5., 0., 1.,\n",
       "         0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 4., 0., 2.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 3., 2., 0., 1.,\n",
       "         0., 0., 0., 2., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 5., 0., 1.,\n",
       "         0., 0., 0., 4., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 1., 0., 1.,\n",
       "         0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 4., 1., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 3., 1., 0.,\n",
       "         0., 0., 0., 2., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 3., 2., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 3., 3., 1., 0.,\n",
       "         0., 0., 0., 3., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "         0., 0.]], dtype=float32)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_state_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_player_index': 0,\n",
       " 'general': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 'source_card_vectors': array([], dtype=float32),\n",
       " 'target_card_vectors': array([[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "         0., 0.]], dtype=float32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chosen_action_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_player_index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(vec, pad, dim):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    if pad > vec.size(dim):\n",
    "        pad_size = list(vec.shape)\n",
    "        pad_size[dim] = pad - vec.size(dim)\n",
    "        return torch.cat([vec, torch.zeros(*pad_size)], dim=dim)\n",
    "    return torch.from_numpy(vec.numpy().take(torch.arange(pad), axis=dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        player_dataset: List[Dict],\n",
    "        zone_vector_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.player_dataset = player_dataset\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.player_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        item_dict = self.player_dataset[idx]\n",
    "        action_history = item_dict[\"action_history\"]\n",
    "        current_game_state = item_dict[\"current_game_state\"]\n",
    "        possible_actions = item_dict[\"possible_actions\"]\n",
    "        chosen_action_index = item_dict[\"chosen_action_index\"]\n",
    "\n",
    "        action_history_vectors = self.__get_action_history_vectors(action_history)\n",
    "        current_game_state_vectors = self.__get_current_game_state_vectors(current_game_state)\n",
    "        possible_actions_vectors = self.__get_possible_actions_vectors(possible_actions)\n",
    "        target_action = self.__get_target_action(\n",
    "            n_possible_actions=len(possible_actions),\n",
    "            chosen_action_index=chosen_action_index\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_history_vectors,\n",
    "            current_game_state_vectors,\n",
    "            possible_actions_vectors,\n",
    "            target_action\n",
    "        )\n",
    "\n",
    "    def __action_list_to_tensors(self, action_list: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_list:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_list_vectors\n",
    "        - general: (n_actions, action_dim)\n",
    "        - source_card_vectors: (n_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        general_vectors = []\n",
    "        source_card_vectors = []\n",
    "        target_card_vectors = []\n",
    "\n",
    "        for action_dict in action_list:\n",
    "            general = torch.from_numpy(action_dict[\"general\"])\n",
    "            source_cards = torch.from_numpy(action_dict[\"source_card_vectors\"])\n",
    "            if len(source_cards) == 0:\n",
    "                source_cards = torch.zeros(size=(self.max_n_action_source_cards, self.zone_vector_dim))\n",
    "            else:\n",
    "                source_cards = pad_tensor(\n",
    "                    source_cards,\n",
    "                    pad=self.max_n_action_source_cards,\n",
    "                    dim=0\n",
    "                )\n",
    "\n",
    "            target_cards = torch.from_numpy(action_dict[\"target_card_vectors\"])\n",
    "            if len(target_cards) == 0:\n",
    "                target_cards = torch.zeros(size=(self.max_n_action_target_cards, self.zone_vector_dim))\n",
    "            else:\n",
    "                target_cards = pad_tensor(\n",
    "                    target_cards,\n",
    "                    pad=self.max_n_action_target_cards,\n",
    "                    dim=0\n",
    "                )\n",
    "\n",
    "            general_vectors.append(general[None])\n",
    "            source_card_vectors.append(source_cards[None])\n",
    "            target_card_vectors.append(target_cards[None])\n",
    "\n",
    "        return {\n",
    "            \"general\": torch.cat(general_vectors, dim=0).float(),\n",
    "            \"source_card_vectors\": torch.cat(source_card_vectors, dim=0).float(),\n",
    "            \"target_card_vectors\": torch.cat(target_card_vectors, dim=0).float(),\n",
    "        }\n",
    "\n",
    "    def __get_action_history_vectors(self, action_history: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_history:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * history_size\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_history_vectors\n",
    "        - general: (history_size, action_dim)\n",
    "        - source_card_vectors: (history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=action_history)\n",
    "\n",
    "    def __get_current_game_state_vectors(self, current_game_state: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        current_game_state:\n",
    "        {\n",
    "            global: (global_dim,)\n",
    "            players: (n_players, player_dim)\n",
    "            zones: (n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Return:\n",
    "\n",
    "        current_game_state_vectors:\n",
    "        - global: (global_dim,)\n",
    "        - players: (n_players, player_dim)\n",
    "        - zones: (max_n_zone_vectors, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"global\": torch.from_numpy(current_game_state[\"global\"]).float(),\n",
    "            \"players\": torch.from_numpy(current_game_state[\"players\"]).float(),\n",
    "            \"zones\": pad_tensor(\n",
    "                torch.from_numpy(current_game_state[\"zones\"]),\n",
    "                pad=self.max_n_zone_vectors,\n",
    "                dim=0\n",
    "            ).float()\n",
    "        }\n",
    "\n",
    "    def __get_possible_actions_vectors(self, possible_actions: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        possible_actions:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_possible_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        possible_actions_vectors:\n",
    "        - general: (n_possible_actions, action_dim)\n",
    "        - source_card_vectors: (n_possible_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_possible_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=possible_actions)\n",
    "\n",
    "    def __get_target_action(self, n_possible_actions: int, chosen_action_index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return:\n",
    "\n",
    "        target_action: (n_possible_actions,)\n",
    "        \"\"\"\n",
    "        target_action = torch.zeros(n_possible_actions).float()\n",
    "        target_action[chosen_action_index] = 1\n",
    "        return target_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_zone_vectors = 120\n",
    "max_n_action_source_cards = 10\n",
    "max_n_action_target_cards = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_dataset = DeepLearningDataset(\n",
    "    player_dataset=dataset,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(deep_learning_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history_vectors, current_game_state_vectors, possible_actions_vectors, target_action = deep_learning_dataset[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['general', 'source_card_vectors', 'target_card_vectors'])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 31])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history_vectors[\"general\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 34])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history_vectors[\"source_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 34])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_history_vectors[\"target_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['global', 'players', 'zones'])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_game_state_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_game_state_vectors[\"global\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_game_state_vectors[\"players\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([120, 34])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_game_state_vectors[\"zones\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['general', 'source_card_vectors', 'target_card_vectors'])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 31])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions_vectors[\"general\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 34])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions_vectors[\"source_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 34])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions_vectors[\"target_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1.])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source_player_index': 0,\n",
       "  'general': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'source_card_vectors': array([], dtype=float32),\n",
       "  'target_card_vectors': array([], dtype=float32)},\n",
       " {'source_player_index': 0,\n",
       "  'general': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'source_card_vectors': array([], dtype=float32),\n",
       "  'target_card_vectors': array([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 2., 3., 2., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "          0., 0.]], dtype=float32)},\n",
       " {'source_player_index': 0,\n",
       "  'general': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  'source_card_vectors': array([], dtype=float32),\n",
       "  'target_card_vectors': array([[1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
       "          0., 0.]], dtype=float32)}]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_possible_actions_collate_fn(\n",
    "    samples: List[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]]\n",
    ") -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "    batch_action_history_vectors = {}\n",
    "    batch_current_game_state_vectors = {}\n",
    "    batch_possible_actions_vectors = {}\n",
    "    batch_target_action = []\n",
    "\n",
    "    max_n_possible_actions = max([sample[2][\"general\"].shape[0] for sample in samples])\n",
    "\n",
    "    for sample in samples:\n",
    "        for key, tensor in sample[0].items():\n",
    "            if key not in batch_action_history_vectors:\n",
    "                batch_action_history_vectors[key] = []\n",
    "            batch_action_history_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[1].items():\n",
    "            if key not in batch_current_game_state_vectors:\n",
    "                batch_current_game_state_vectors[key] = []\n",
    "            batch_current_game_state_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[2].items():\n",
    "            if key not in batch_possible_actions_vectors:\n",
    "                batch_possible_actions_vectors[key] = []\n",
    "            tensor = pad_tensor(tensor, pad=max_n_possible_actions, dim=0)\n",
    "            batch_possible_actions_vectors[key].append(tensor[None])\n",
    "        batch_target_action.append(\n",
    "            pad_tensor(\n",
    "                sample[3],\n",
    "                pad=max_n_possible_actions,\n",
    "                dim=0\n",
    "            )[None]\n",
    "        )\n",
    "\n",
    "    for key, tensors in batch_action_history_vectors.items():\n",
    "        batch_action_history_vectors[key] = torch.cat(tensors, dim=0)\n",
    "    for key, tensors in batch_current_game_state_vectors.items():\n",
    "        batch_current_game_state_vectors[key] = torch.cat(tensors, dim=0)\n",
    "    for key, tensors in batch_possible_actions_vectors.items():\n",
    "        batch_possible_actions_vectors[key] = torch.cat(tensors, dim=0)\n",
    "    batch_target_action = torch.cat(batch_target_action, dim=0)\n",
    "    \n",
    "    return (\n",
    "        batch_action_history_vectors,\n",
    "        batch_current_game_state_vectors,\n",
    "        batch_possible_actions_vectors,\n",
    "        batch_target_action\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    deep_learning_dataset,\n",
    "    batch_size=5,\n",
    "    collate_fn=pad_possible_actions_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors, batch_target_action = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['general', 'source_card_vectors', 'target_card_vectors'])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_action_history_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 31])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_action_history_vectors[\"general\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 10, 34])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_action_history_vectors[\"source_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 10, 34])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_action_history_vectors[\"target_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['global', 'players', 'zones'])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_current_game_state_vectors.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_current_game_state_vectors[\"global\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 8])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_current_game_state_vectors[\"players\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 120, 34])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_current_game_state_vectors[\"zones\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 31])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_possible_actions_vectors[\"general\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 10, 34])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_possible_actions_vectors[\"source_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 10, 34])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_possible_actions_vectors[\"target_card_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_target_action"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepLearningScorer(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = torch.nn.BCELoss()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        batch_action_history_vectors:\n",
    "        {\n",
    "            general: (batch_size, history_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_current_game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_possible_actions_vectors:\n",
    "        {\n",
    "            general: (batch_size, max_n_possible_actions_in_batch, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Returns:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __step(self, batch, batch_idx, base_metric_name):\n",
    "        batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors, batch_target_action = batch\n",
    "        batch_predicted_target_action = self.forward(\n",
    "            batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors\n",
    "        )\n",
    "        batch_loss = self.loss(batch_predicted_target_action, batch_target_action)  # FIXME: Mulitply with a mask\n",
    "        self.log(f\"{base_metric_name}_loss\", batch_loss, on_epoch=True)\n",
    "        return batch_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"training\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"validation\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        batch_game_state_vectors, batch_action_vectors = batch\n",
    "        batch_predicted_scores = self.forward(\n",
    "            batch_game_state_vectors=batch_game_state_vectors, batch_action_vectors=batch_action_vectors\n",
    "        )\n",
    "        return batch_predicted_scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def get_n_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProcessingBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.general_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.action_general_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.card_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, action_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - action_vectors:\n",
    "        {\n",
    "            general: (batch_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - action_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = action_vectors[\"general\"].shape[0]\n",
    "\n",
    "        action_general_embedding = self.__get_action_general_embedding(action_vectors[\"general\"])\n",
    "        action_source_card_embeddings = self.__get_action_source_card_embeddings(action_vectors[\"source_card_vectors\"])\n",
    "        action_target_card_embeddings = self.__get_action_target_card_embeddings(action_vectors[\"target_card_vectors\"])\n",
    "\n",
    "        action_embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim)\n",
    "\n",
    "        action_embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                action_embedding_for_prediction,\n",
    "                action_general_embedding[:, None],\n",
    "                action_source_card_embeddings,\n",
    "                action_target_card_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        action_embeddings_sequence_after_transformer = self.transformer_encoder(action_embeddings_sequence)\n",
    "\n",
    "        return action_embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_action_general_embedding(self, action_general_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_general_vector.shape[0]\n",
    "        action_general_embedding = self.general_mlp(action_general_vector)\n",
    "        action_general_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1)\n",
    "        return torch.cat([action_general_embedding, action_general_embedding_type], dim=1)\n",
    "\n",
    "    def __get_action_source_card_embeddings(self, action_source_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_source_card_vectors.shape[0]\n",
    "        action_source_card_embeddings = self.card_mlp(action_source_card_vectors)\n",
    "        action_source_card_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.max_n_action_source_cards, 1)\n",
    "        return torch.cat([action_source_card_embeddings, action_source_card_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_action_target_card_embeddings(self, action_target_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_target_card_vectors.shape[0]\n",
    "        action_target_card_embeddings = self.card_mlp(action_target_card_vectors)\n",
    "        action_target_card_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_action_target_cards, 1)\n",
    "        return torch.cat([action_target_card_embeddings, action_target_card_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class GameStateProcessingBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.global_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.game_state_global_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.player_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.player_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.zone_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, game_state_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - game_state_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = game_state_vectors[\"global\"].shape[0]\n",
    "\n",
    "        global_embedding = self.__get_global_embedding(game_state_vectors[\"global\"])\n",
    "        player_embeddings = self.__get_player_embeddings(game_state_vectors[\"players\"])\n",
    "        zone_embeddings = self.__get_zone_embeddings(game_state_vectors[\"zones\"])\n",
    "\n",
    "        embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim)\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                embedding_for_prediction,\n",
    "                global_embedding[:, None],\n",
    "                player_embeddings,\n",
    "                zone_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        return embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_global_embedding(self, game_state_global_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_global_vector.shape[0]\n",
    "        global_embedding = self.global_mlp(game_state_global_vector)\n",
    "        global_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1)\n",
    "        return torch.cat([global_embedding, global_embedding_type], dim=1)\n",
    "\n",
    "    def __get_player_embeddings(self, game_state_player_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_player_vectors.shape[0]\n",
    "        player_embeddings = self.player_mlp(game_state_player_vectors)\n",
    "        player_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.n_players, 1)\n",
    "        return torch.cat([player_embeddings, player_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_zone_embeddings(self, game_state_zone_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_zone_vectors.shape[0]\n",
    "        zone_embeddings = self.zone_mlp(game_state_zone_vectors)\n",
    "        zone_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_zone_vectors, 1)\n",
    "        return torch.cat([zone_embeddings, zone_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class ClassificationBlock(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.input_dim + 3,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "        self.final_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_dim + 3, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_action_history_embeddings: torch.Tensor,\n",
    "        batch_current_game_state_embedding: torch.Tensor,\n",
    "        batch_possible_actions_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - batch_action_history_embeddings: (batch_size, history_size, input_dim)\n",
    "        - batch_current_game_state_embedding: (batch_size, input_dim)\n",
    "        - batch_possible_actions_embeddings: (batch_size, max_n_possible_actions_in_batch, input_dim)\n",
    "\n",
    "        Outputs:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "\n",
    "        batch_action_history_embeddings = self.__prepare_action_history_embeddings(\n",
    "            batch_action_history_embeddings\n",
    "        )\n",
    "        batch_current_game_state_embedding = self.__prepare_current_game_state_embedding(\n",
    "            batch_current_game_state_embedding\n",
    "        )\n",
    "        batch_possible_actions_embeddings = self.__prepare_possible_actions_embeddings(\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                batch_action_history_embeddings,\n",
    "                batch_current_game_state_embedding,\n",
    "                batch_possible_actions_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        possible_actions_embeddings = embeddings_sequence_after_transformer[:, -max_n_possible_actions_in_batch:]\n",
    "\n",
    "        predicted_target_action = torch.softmax(\n",
    "            self.final_mlp(possible_actions_embeddings)[..., 0],\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        return predicted_target_action\n",
    "\n",
    "    def __prepare_action_history_embeddings(self, batch_action_history_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_action_history_embeddings.shape[0]\n",
    "        history_size = batch_action_history_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[1.0, 0.0, 0.0]]]).repeat(batch_size, history_size, 1)\n",
    "        return torch.cat([batch_action_history_embeddings, embeddings_type], dim=2)\n",
    "\n",
    "    def __prepare_current_game_state_embedding(self, batch_current_game_state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_current_game_state_embedding.shape[0]\n",
    "        embedding_type = torch.tensor([[0.0, 1.0, 0.0]]).repeat(batch_size, 1)\n",
    "        return torch.cat([batch_current_game_state_embedding, embedding_type], dim=1)[:, None]\n",
    "\n",
    "    def __prepare_possible_actions_embeddings(self, batch_possible_actions_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_possible_actions_embeddings.shape[0]\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, max_n_possible_actions_in_batch, 1)\n",
    "        return torch.cat([batch_possible_actions_embeddings, embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class DeepLearningScorerV1(BaseDeepLearningScorer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        embedding_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Modules\n",
    "        self.action_processing_block = ActionProcessingBlock(\n",
    "            action_general_dim=self.action_general_dim,\n",
    "            max_n_action_source_cards=self.max_n_action_source_cards,\n",
    "            max_n_action_target_cards=self.max_n_action_target_cards,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=1,\n",
    "            transformer_n_heads=1,\n",
    "            transformer_dim_feedforward=128,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        self.game_state_processing_block = GameStateProcessingBlock(\n",
    "            game_state_global_dim=self.game_state_global_dim,\n",
    "            n_players=self.n_players,\n",
    "            player_dim=self.player_dim,\n",
    "            max_n_zone_vectors=self.max_n_zone_vectors,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=1,\n",
    "            transformer_n_heads=1,\n",
    "            transformer_dim_feedforward=128,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        self.classification_block = ClassificationBlock(\n",
    "            input_dim=self.embedding_dim,\n",
    "            transformer_n_layers=1,\n",
    "            transformer_n_heads=1,\n",
    "            transformer_dim_feedforward=128,\n",
    "            dropout=0.0\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        batch_action_history_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_action_history_vectors,\n",
    "            n_actions=batch_action_history_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_current_game_state_embedding = self.game_state_processing_block(batch_current_game_state_vectors)\n",
    "\n",
    "        batch_possible_actions_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_possible_actions_vectors,\n",
    "            n_actions=batch_possible_actions_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_predicted_target_action = self.classification_block(\n",
    "            batch_action_history_embeddings,\n",
    "            batch_current_game_state_embedding,\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        return batch_predicted_target_action\n",
    "\n",
    "    def __process_action_list(self, batch_action_list_vectors: torch.Tensor, n_actions: int) -> torch.Tensor:\n",
    "        action_embeddings = []\n",
    "        for i in range(n_actions):\n",
    "            one_action_vectors = {key: tensor[:, i] for key, tensor in batch_action_list_vectors.items()}\n",
    "            action_embedding = self.action_processing_block(one_action_vectors)\n",
    "            action_embeddings.append(action_embedding[:, None])\n",
    "        return torch.cat(action_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLearningScorerV1(\n",
    "    game_state_global_dim=game_state_global_dim,\n",
    "    n_players=n_players,\n",
    "    player_dim=player_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    action_general_dim=action_general_dim,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    embedding_dim=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predicted_target_action = model(\n",
    "    batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1953, 0.2037, 0.2037, 0.2037, 0.1936],\n",
       "        [0.2006, 0.2028, 0.1989, 0.1989, 0.1989],\n",
       "        [0.2014, 0.1997, 0.1997, 0.1997, 0.1997],\n",
       "        [0.2006, 0.2028, 0.1989, 0.1989, 0.1989],\n",
       "        [0.1946, 0.2029, 0.2029, 0.2029, 0.1968]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_predicted_target_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
