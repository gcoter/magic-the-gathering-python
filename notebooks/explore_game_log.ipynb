{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from abc import abstractmethod\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state_global_dim = 2\n",
    "n_players = 2\n",
    "player_dim = 8\n",
    "action_general_dim = 31\n",
    "zone_vector_dim = 34\n",
    "\n",
    "max_n_zone_vectors = 120\n",
    "max_n_action_source_cards = 10\n",
    "max_n_action_target_cards = 10\n",
    "\n",
    "embedding_dim = 64\n",
    "transformer_n_layers = 10\n",
    "transformer_n_heads = 8\n",
    "transformer_dim_feedforward = 128\n",
    "dropout = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "mlflow.log_param(\"transformer_n_layers\", transformer_n_layers)\n",
    "mlflow.log_param(\"transformer_n_heads\", transformer_n_heads)\n",
    "mlflow.log_param(\"transformer_dim_feedforward\", transformer_dim_feedforward)\n",
    "mlflow.log_param(\"dropout\", dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read one pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_logs_folder_path = \"../data/game_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_from_pickles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(game_logs_folder_path):\n",
    "    game_log_file_path = os.path.join(game_logs_folder_path, file_name)\n",
    "    print(f\"Read game logs from '{game_log_file_path}'\")\n",
    "    with open(game_log_file_path, \"rb\") as f:\n",
    "        data_dict = pickle.load(f)\n",
    "        dataset_from_pickles.extend(data_dict[\"dataset\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_from_pickles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(\n",
    "    vec: torch.Tensor,\n",
    "    pad: int,\n",
    "    dim: int,\n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    if pad > vec.size(dim):\n",
    "        pad_size = list(vec.shape)\n",
    "        pad_size[dim] = pad - vec.size(dim)\n",
    "        return torch.cat([vec.to(device), torch.zeros(*pad_size).to(device)], dim=dim)\n",
    "    return torch.from_numpy(vec.cpu().numpy().take(torch.arange(pad), axis=dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        player_dataset: List[Dict],\n",
    "        zone_vector_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.player_dataset = player_dataset\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.player_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        item_dict = self.player_dataset[idx]\n",
    "        action_history = item_dict[\"action_history\"]\n",
    "        current_game_state = item_dict[\"current_game_state\"]\n",
    "        possible_actions = item_dict[\"possible_actions\"]\n",
    "        chosen_action_index = item_dict[\"chosen_action_index\"]\n",
    "\n",
    "        action_history_vectors = self.__get_action_history_vectors(action_history)\n",
    "        current_game_state_vectors = self.__get_current_game_state_vectors(current_game_state)\n",
    "        possible_actions_vectors = self.__get_possible_actions_vectors(possible_actions)\n",
    "        target_action = self.__get_target_action(\n",
    "            n_possible_actions=len(possible_actions),\n",
    "            chosen_action_index=chosen_action_index\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_history_vectors,\n",
    "            current_game_state_vectors,\n",
    "            possible_actions_vectors,\n",
    "            target_action\n",
    "        )\n",
    "\n",
    "    def __action_list_to_tensors(self, action_list: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_list:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_list_vectors\n",
    "        - general: (n_actions, action_dim)\n",
    "        - source_card_vectors: (n_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        general_vectors = []\n",
    "        source_card_vectors = []\n",
    "        target_card_vectors = []\n",
    "\n",
    "        for action_dict in action_list:\n",
    "            general = torch.from_numpy(action_dict[\"general\"]).to(self.device)\n",
    "            source_cards = torch.from_numpy(action_dict[\"source_card_vectors\"])\n",
    "            if len(source_cards) == 0:\n",
    "                source_cards = torch.zeros(size=(self.max_n_action_source_cards, self.zone_vector_dim)).to(self.device)\n",
    "            else:\n",
    "                source_cards = pad_tensor(\n",
    "                    source_cards,\n",
    "                    pad=self.max_n_action_source_cards,\n",
    "                    dim=0,\n",
    "                    device=self.device\n",
    "                ).to(self.device)\n",
    "\n",
    "            target_cards = torch.from_numpy(action_dict[\"target_card_vectors\"])\n",
    "            if len(target_cards) == 0:\n",
    "                target_cards = torch.zeros(size=(self.max_n_action_target_cards, self.zone_vector_dim)).to(self.device)\n",
    "            else:\n",
    "                target_cards = pad_tensor(\n",
    "                    target_cards,\n",
    "                    pad=self.max_n_action_target_cards,\n",
    "                    dim=0,\n",
    "                    device=self.device\n",
    "                ).to(self.device)\n",
    "\n",
    "            general_vectors.append(general[None])\n",
    "            source_card_vectors.append(source_cards[None])\n",
    "            target_card_vectors.append(target_cards[None])\n",
    "\n",
    "        return {\n",
    "            \"general\": torch.cat(general_vectors, dim=0).float().to(self.device),\n",
    "            \"source_card_vectors\": torch.cat(source_card_vectors, dim=0).float().to(self.device),\n",
    "            \"target_card_vectors\": torch.cat(target_card_vectors, dim=0).float().to(self.device),\n",
    "        }\n",
    "\n",
    "    def __get_action_history_vectors(self, action_history: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_history:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * history_size\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_history_vectors\n",
    "        - general: (history_size, action_dim)\n",
    "        - source_card_vectors: (history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=action_history)\n",
    "\n",
    "    def __get_current_game_state_vectors(self, current_game_state: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        current_game_state:\n",
    "        {\n",
    "            global: (global_dim,)\n",
    "            players: (n_players, player_dim)\n",
    "            zones: (n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Return:\n",
    "\n",
    "        current_game_state_vectors:\n",
    "        - global: (global_dim,)\n",
    "        - players: (n_players, player_dim)\n",
    "        - zones: (max_n_zone_vectors, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"global\": torch.from_numpy(current_game_state[\"global\"]).float().to(self.device),\n",
    "            \"players\": torch.from_numpy(current_game_state[\"players\"]).float().to(self.device),\n",
    "            \"zones\": pad_tensor(\n",
    "                torch.from_numpy(current_game_state[\"zones\"]),\n",
    "                pad=self.max_n_zone_vectors,\n",
    "                dim=0,\n",
    "                device=self.device\n",
    "            ).float().to(self.device)\n",
    "        }\n",
    "\n",
    "    def __get_possible_actions_vectors(self, possible_actions: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        possible_actions:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_possible_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        possible_actions_vectors:\n",
    "        - general: (n_possible_actions, action_dim)\n",
    "        - source_card_vectors: (n_possible_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_possible_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=possible_actions)\n",
    "\n",
    "    def __get_target_action(self, n_possible_actions: int, chosen_action_index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return:\n",
    "\n",
    "        target_action: (n_possible_actions,)\n",
    "        \"\"\"\n",
    "        target_action = torch.zeros(n_possible_actions).float().to(self.device)\n",
    "        target_action[chosen_action_index] = 1\n",
    "        return target_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_dataset = DeepLearningDataset(\n",
    "    player_dataset=dataset_from_pickles,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_possible_actions_collate_fn(\n",
    "    samples: List[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]],\n",
    "    device\n",
    ") -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "    batch_action_history_vectors = {}\n",
    "    batch_current_game_state_vectors = {}\n",
    "    batch_possible_actions_vectors = {}\n",
    "    batch_target_action = []\n",
    "\n",
    "    max_n_possible_actions = max([sample[2][\"general\"].shape[0] for sample in samples])\n",
    "\n",
    "    for sample in samples:\n",
    "        for key, tensor in sample[0].items():\n",
    "            if key not in batch_action_history_vectors:\n",
    "                batch_action_history_vectors[key] = []\n",
    "            batch_action_history_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[1].items():\n",
    "            if key not in batch_current_game_state_vectors:\n",
    "                batch_current_game_state_vectors[key] = []\n",
    "            batch_current_game_state_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[2].items():\n",
    "            if key not in batch_possible_actions_vectors:\n",
    "                batch_possible_actions_vectors[key] = []\n",
    "            tensor = pad_tensor(tensor, pad=max_n_possible_actions, dim=0, device=device)\n",
    "            batch_possible_actions_vectors[key].append(tensor[None])\n",
    "        batch_target_action.append(\n",
    "            pad_tensor(\n",
    "                sample[3],\n",
    "                pad=max_n_possible_actions,\n",
    "                dim=0,\n",
    "                device=device\n",
    "            )[None]\n",
    "        )\n",
    "\n",
    "    for key, tensors in batch_action_history_vectors.items():\n",
    "        batch_action_history_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    for key, tensors in batch_current_game_state_vectors.items():\n",
    "        batch_current_game_state_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    for key, tensors in batch_possible_actions_vectors.items():\n",
    "        batch_possible_actions_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    batch_target_action = torch.cat(batch_target_action, dim=0).to(device)\n",
    "    \n",
    "    return (\n",
    "        batch_action_history_vectors,\n",
    "        batch_current_game_state_vectors,\n",
    "        batch_possible_actions_vectors,\n",
    "        batch_target_action\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepLearningScorer(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        batch_action_history_vectors:\n",
    "        {\n",
    "            general: (batch_size, history_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_current_game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_possible_actions_vectors:\n",
    "        {\n",
    "            general: (batch_size, max_n_possible_actions_in_batch, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Returns:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __step(self, batch, batch_idx, base_metric_name):\n",
    "        batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors, batch_target_action = batch\n",
    "        batch_predicted_target_action = self.forward(\n",
    "            batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors\n",
    "        )\n",
    "        batch_loss = self.loss(batch_predicted_target_action, batch_target_action)  # FIXME: Mulitply with a mask\n",
    "        self.log(f\"{base_metric_name}_loss\", batch_loss, on_epoch=True, prog_bar=True)\n",
    "        return batch_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"training\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"validation\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        batch_game_state_vectors, batch_action_vectors = batch\n",
    "        batch_predicted_scores = self.forward(\n",
    "            batch_game_state_vectors=batch_game_state_vectors, batch_action_vectors=batch_action_vectors\n",
    "        )\n",
    "        return batch_predicted_scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def get_n_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProcessingBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.general_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.action_general_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.card_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, action_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - action_vectors:\n",
    "        {\n",
    "            general: (batch_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - action_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = action_vectors[\"general\"].shape[0]\n",
    "\n",
    "        action_general_embedding = self.__get_action_general_embedding(action_vectors[\"general\"])\n",
    "        action_source_card_embeddings = self.__get_action_source_card_embeddings(action_vectors[\"source_card_vectors\"])\n",
    "        action_target_card_embeddings = self.__get_action_target_card_embeddings(action_vectors[\"target_card_vectors\"])\n",
    "\n",
    "        action_embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim).to(action_general_embedding)\n",
    "\n",
    "        action_embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                action_embedding_for_prediction,\n",
    "                action_general_embedding[:, None],\n",
    "                action_source_card_embeddings,\n",
    "                action_target_card_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        action_embeddings_sequence_after_transformer = self.transformer_encoder(action_embeddings_sequence)\n",
    "\n",
    "        return action_embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_action_general_embedding(self, action_general_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_general_vector.shape[0]\n",
    "        action_general_embedding = self.general_mlp(action_general_vector)\n",
    "        action_general_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1).to(action_general_embedding)\n",
    "        return torch.cat([action_general_embedding, action_general_embedding_type], dim=1)\n",
    "\n",
    "    def __get_action_source_card_embeddings(self, action_source_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_source_card_vectors.shape[0]\n",
    "        action_source_card_embeddings = self.card_mlp(action_source_card_vectors)\n",
    "        action_source_card_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.max_n_action_source_cards, 1).to(action_source_card_embeddings)\n",
    "        return torch.cat([action_source_card_embeddings, action_source_card_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_action_target_card_embeddings(self, action_target_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_target_card_vectors.shape[0]\n",
    "        action_target_card_embeddings = self.card_mlp(action_target_card_vectors)\n",
    "        action_target_card_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_action_target_cards, 1).to(action_target_card_embeddings)\n",
    "        return torch.cat([action_target_card_embeddings, action_target_card_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class GameStateProcessingBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.global_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.game_state_global_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.player_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.player_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.zone_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, game_state_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - game_state_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = game_state_vectors[\"global\"].shape[0]\n",
    "\n",
    "        global_embedding = self.__get_global_embedding(game_state_vectors[\"global\"])\n",
    "        player_embeddings = self.__get_player_embeddings(game_state_vectors[\"players\"])\n",
    "        zone_embeddings = self.__get_zone_embeddings(game_state_vectors[\"zones\"])\n",
    "\n",
    "        embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim).to(global_embedding)\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                embedding_for_prediction,\n",
    "                global_embedding[:, None],\n",
    "                player_embeddings,\n",
    "                zone_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        return embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_global_embedding(self, game_state_global_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_global_vector.shape[0]\n",
    "        global_embedding = self.global_mlp(game_state_global_vector)\n",
    "        global_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1).to(global_embedding)\n",
    "        return torch.cat([global_embedding, global_embedding_type], dim=1)\n",
    "\n",
    "    def __get_player_embeddings(self, game_state_player_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_player_vectors.shape[0]\n",
    "        player_embeddings = self.player_mlp(game_state_player_vectors)\n",
    "        player_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.n_players, 1).to(player_embeddings)\n",
    "        return torch.cat([player_embeddings, player_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_zone_embeddings(self, game_state_zone_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_zone_vectors.shape[0]\n",
    "        zone_embeddings = self.zone_mlp(game_state_zone_vectors)\n",
    "        zone_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_zone_vectors, 1).to(zone_embeddings)\n",
    "        return torch.cat([zone_embeddings, zone_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class ClassificationBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.preprocessing_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_dim + 3, out_features=self.input_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.input_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "        self.final_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_dim, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_action_history_embeddings: torch.Tensor,\n",
    "        batch_current_game_state_embedding: torch.Tensor,\n",
    "        batch_possible_actions_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - batch_action_history_embeddings: (batch_size, history_size, input_dim)\n",
    "        - batch_current_game_state_embedding: (batch_size, input_dim)\n",
    "        - batch_possible_actions_embeddings: (batch_size, max_n_possible_actions_in_batch, input_dim)\n",
    "\n",
    "        Outputs:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "\n",
    "        batch_action_history_embeddings = self.__prepare_action_history_embeddings(\n",
    "            batch_action_history_embeddings\n",
    "        )\n",
    "        batch_current_game_state_embedding = self.__prepare_current_game_state_embedding(\n",
    "            batch_current_game_state_embedding\n",
    "        )\n",
    "        batch_possible_actions_embeddings = self.__prepare_possible_actions_embeddings(\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                batch_action_history_embeddings,\n",
    "                batch_current_game_state_embedding,\n",
    "                batch_possible_actions_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        possible_actions_embeddings = embeddings_sequence_after_transformer[:, -max_n_possible_actions_in_batch:]\n",
    "\n",
    "        predicted_target_action = torch.softmax(\n",
    "            self.final_mlp(possible_actions_embeddings)[..., 0],\n",
    "            dim=-1\n",
    "        )\n",
    "\n",
    "        return predicted_target_action\n",
    "\n",
    "    def __prepare_action_history_embeddings(self, batch_action_history_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_action_history_embeddings.shape[0]\n",
    "        history_size = batch_action_history_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[1.0, 0.0, 0.0]]]).repeat(batch_size, history_size, 1).to(batch_action_history_embeddings)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_action_history_embeddings, embeddings_type], dim=2))\n",
    "\n",
    "    def __prepare_current_game_state_embedding(self, batch_current_game_state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_current_game_state_embedding.shape[0]\n",
    "        embedding_type = torch.tensor([[0.0, 1.0, 0.0]]).repeat(batch_size, 1).to(batch_current_game_state_embedding)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_current_game_state_embedding, embedding_type], dim=1)[:, None])\n",
    "\n",
    "    def __prepare_possible_actions_embeddings(self, batch_possible_actions_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_possible_actions_embeddings.shape[0]\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, max_n_possible_actions_in_batch, 1).to(batch_possible_actions_embeddings)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_possible_actions_embeddings, embeddings_type], dim=2))\n",
    "\n",
    "\n",
    "class DeepLearningScorerV1(BaseDeepLearningScorer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_n_layers: int,\n",
    "        transformer_n_heads: int,\n",
    "        transformer_dim_feedforward: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.action_processing_block = ActionProcessingBlock(\n",
    "            action_general_dim=self.action_general_dim,\n",
    "            max_n_action_source_cards=self.max_n_action_source_cards,\n",
    "            max_n_action_target_cards=self.max_n_action_target_cards,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=1,\n",
    "            transformer_dim_feedforward=128,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.game_state_processing_block = GameStateProcessingBlock(\n",
    "            game_state_global_dim=self.game_state_global_dim,\n",
    "            n_players=self.n_players,\n",
    "            player_dim=self.player_dim,\n",
    "            max_n_zone_vectors=self.max_n_zone_vectors,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=self.transformer_n_heads,\n",
    "            transformer_dim_feedforward=self.transformer_dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.classification_block = ClassificationBlock(\n",
    "            input_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=self.transformer_n_heads,\n",
    "            transformer_dim_feedforward=self.transformer_dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        batch_action_history_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_action_history_vectors,\n",
    "            n_actions=batch_action_history_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_current_game_state_embedding = self.game_state_processing_block(batch_current_game_state_vectors)\n",
    "\n",
    "        batch_possible_actions_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_possible_actions_vectors,\n",
    "            n_actions=batch_possible_actions_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_predicted_target_action = self.classification_block(\n",
    "            batch_action_history_embeddings,\n",
    "            batch_current_game_state_embedding,\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        return batch_predicted_target_action\n",
    "\n",
    "    def __process_action_list(self, batch_action_list_vectors: torch.Tensor, n_actions: int) -> torch.Tensor:\n",
    "        action_embeddings = []\n",
    "        for i in range(n_actions):\n",
    "            one_action_vectors = {key: tensor[:, i] for key, tensor in batch_action_list_vectors.items()}\n",
    "            action_embedding = self.action_processing_block(one_action_vectors)\n",
    "            action_embeddings.append(action_embedding[:, None])\n",
    "        return torch.cat(action_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLearningScorerV1(\n",
    "    game_state_global_dim=game_state_global_dim,\n",
    "    n_players=n_players,\n",
    "    player_dim=player_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    action_general_dim=action_general_dim,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    embedding_dim=embedding_dim,\n",
    "    transformer_n_layers=transformer_n_layers,\n",
    "    transformer_n_heads=transformer_n_heads,\n",
    "    transformer_dim_feedforward=transformer_dim_feedforward,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    deep_learning_dataset,\n",
    "    lengths=[\n",
    "        0.8,\n",
    "        0.2,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda samples: pad_possible_actions_collate_fn(samples=samples, device=device)\n",
    ")\n",
    "validation_data_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda samples: pad_possible_actions_collate_fn(samples=samples, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_path = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(model_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=model_folder_path,\n",
    "        filename=\"deep_learning_scorer\",\n",
    "        monitor=\"validation_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=False\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"validation_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=10,\n",
    "        verbose=False\n",
    "    ),\n",
    "]\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    devices=\"auto\",\n",
    "    deterministic=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=training_data_loader,\n",
    "    val_dataloaders=validation_data_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
