{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "from torch.utils.data import Dataset\n",
    "from abc import abstractmethod\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/25 11:20:14 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n"
     ]
    }
   ],
   "source": [
    "mlflow.pytorch.autolog()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_state_global_dim = 2\n",
    "n_players = 2\n",
    "player_dim = 8\n",
    "action_general_dim = 31\n",
    "zone_vector_dim = 34\n",
    "\n",
    "max_n_zone_vectors = 120\n",
    "max_n_action_source_cards = 10\n",
    "max_n_action_target_cards = 10\n",
    "\n",
    "embedding_dim = 64\n",
    "transformer_n_layers = 1\n",
    "transformer_n_heads = 8\n",
    "transformer_dim_feedforward = 128\n",
    "dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "mlflow.log_param(\"transformer_n_layers\", transformer_n_layers)\n",
    "mlflow.log_param(\"transformer_n_heads\", transformer_n_heads)\n",
    "mlflow.log_param(\"transformer_dim_feedforward\", transformer_dim_feedforward)\n",
    "mlflow.log_param(\"dropout\", dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read one pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_logs_folder_path = \"../data/game_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_from_pickles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read game logs from '../data/game_logs/game_e7efdda1-10e0-4766-bb9f-0e945b22ab80.pickle'\n",
      "Read game logs from '../data/game_logs/game_c74746b7-a8aa-407f-bcfd-2c2226ee07a5.pickle'\n",
      "Read game logs from '../data/game_logs/game_9f9f5c4b-82a6-4051-8bb7-96b3431d569e.pickle'\n",
      "Read game logs from '../data/game_logs/game_48304f8b-f30f-458e-9044-f5a431df0e28.pickle'\n",
      "Read game logs from '../data/game_logs/game_5f13fda1-438c-4f52-926d-c079367afd3c.pickle'\n",
      "Read game logs from '../data/game_logs/game_aec500b8-ddf8-441b-855d-5110dd441071.pickle'\n",
      "Read game logs from '../data/game_logs/game_d3eac282-9985-4485-bd96-a12e5b0bd6c7.pickle'\n",
      "Read game logs from '../data/game_logs/game_d6f13de5-c6cd-4257-8ef9-40efc6803260.pickle'\n",
      "Read game logs from '../data/game_logs/game_bf1115b0-0485-433a-ab0c-f0ccafde4a12.pickle'\n",
      "Read game logs from '../data/game_logs/game_674fed46-a4cb-443d-bddc-bbd809b30aa6.pickle'\n"
     ]
    }
   ],
   "source": [
    "for file_name in os.listdir(game_logs_folder_path):\n",
    "    game_log_file_path = os.path.join(game_logs_folder_path, file_name)\n",
    "    print(f\"Read game logs from '{game_log_file_path}'\")\n",
    "    with open(game_log_file_path, \"rb\") as f:\n",
    "        data_dict = pickle.load(f)\n",
    "\n",
    "        for item_dict in data_dict[\"dataset\"]:\n",
    "            n_possible_actions = len(item_dict[\"possible_actions\"])\n",
    "            if n_possible_actions >= 2:\n",
    "                dataset_from_pickles.append(item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1269"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_from_pickles)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tensor(\n",
    "    vec: torch.Tensor,\n",
    "    pad: int,\n",
    "    dim: int,\n",
    "    device,\n",
    "    return_pad_size: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        vec - tensor to pad\n",
    "        pad - the size to pad to\n",
    "        dim - dimension to pad\n",
    "\n",
    "    return:\n",
    "        a new tensor padded to 'pad' in dimension 'dim'\n",
    "    \"\"\"\n",
    "    pad_size = 0\n",
    "    if pad > vec.size(dim):\n",
    "        pad_shape = list(vec.shape)\n",
    "        pad_size = pad - vec.size(dim)\n",
    "        pad_shape[dim] = pad_size\n",
    "        padded_tensor = torch.cat([vec.to(device), torch.zeros(*pad_shape).to(device)], dim=dim)\n",
    "    else:\n",
    "        padded_tensor = torch.from_numpy(vec.cpu().numpy().take(torch.arange(pad), axis=dim)).to(device)\n",
    "    if return_pad_size:\n",
    "        return padded_tensor, pad_size\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLearningDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        player_dataset: List[Dict],\n",
    "        zone_vector_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        device\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.player_dataset = player_dataset\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.player_dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "        item_dict = self.player_dataset[idx]\n",
    "        action_history = item_dict[\"action_history\"]\n",
    "        current_game_state = item_dict[\"current_game_state\"]\n",
    "        possible_actions = item_dict[\"possible_actions\"]\n",
    "        chosen_action_index = item_dict[\"chosen_action_index\"]\n",
    "\n",
    "        action_history_vectors = self.__get_action_history_vectors(action_history)\n",
    "        current_game_state_vectors = self.__get_current_game_state_vectors(current_game_state)\n",
    "        possible_actions_vectors = self.__get_possible_actions_vectors(possible_actions)\n",
    "        target_action = self.__get_target_action(\n",
    "            n_possible_actions=len(possible_actions),\n",
    "            chosen_action_index=chosen_action_index\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            action_history_vectors,\n",
    "            current_game_state_vectors,\n",
    "            possible_actions_vectors,\n",
    "            target_action\n",
    "        )\n",
    "\n",
    "    def __action_list_to_tensors(self, action_list: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_list:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_list_vectors\n",
    "        - general: (n_actions, action_dim)\n",
    "        - source_card_vectors: (n_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        general_vectors = []\n",
    "        source_card_vectors = []\n",
    "        target_card_vectors = []\n",
    "\n",
    "        for action_dict in action_list:\n",
    "            general = torch.from_numpy(action_dict[\"general\"]).to(self.device)\n",
    "            source_cards = torch.from_numpy(action_dict[\"source_card_vectors\"])\n",
    "            if len(source_cards) == 0:\n",
    "                source_cards = torch.zeros(size=(self.max_n_action_source_cards, self.zone_vector_dim)).to(self.device)\n",
    "            else:\n",
    "                source_cards = pad_tensor(\n",
    "                    source_cards,\n",
    "                    pad=self.max_n_action_source_cards,\n",
    "                    dim=0,\n",
    "                    device=self.device\n",
    "                ).to(self.device)\n",
    "\n",
    "            target_cards = torch.from_numpy(action_dict[\"target_card_vectors\"])\n",
    "            if len(target_cards) == 0:\n",
    "                target_cards = torch.zeros(size=(self.max_n_action_target_cards, self.zone_vector_dim)).to(self.device)\n",
    "            else:\n",
    "                target_cards = pad_tensor(\n",
    "                    target_cards,\n",
    "                    pad=self.max_n_action_target_cards,\n",
    "                    dim=0,\n",
    "                    device=self.device\n",
    "                ).to(self.device)\n",
    "\n",
    "            general_vectors.append(general[None])\n",
    "            source_card_vectors.append(source_cards[None])\n",
    "            target_card_vectors.append(target_cards[None])\n",
    "\n",
    "        return {\n",
    "            \"general\": torch.cat(general_vectors, dim=0).float().to(self.device),\n",
    "            \"source_card_vectors\": torch.cat(source_card_vectors, dim=0).float().to(self.device),\n",
    "            \"target_card_vectors\": torch.cat(target_card_vectors, dim=0).float().to(self.device),\n",
    "        }\n",
    "\n",
    "    def __get_action_history_vectors(self, action_history: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        action_history:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * history_size\n",
    "\n",
    "        Return:\n",
    "\n",
    "        action_history_vectors\n",
    "        - general: (history_size, action_dim)\n",
    "        - source_card_vectors: (history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=action_history)\n",
    "\n",
    "    def __get_current_game_state_vectors(self, current_game_state: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        current_game_state:\n",
    "        {\n",
    "            global: (global_dim,)\n",
    "            players: (n_players, player_dim)\n",
    "            zones: (n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Return:\n",
    "\n",
    "        current_game_state_vectors:\n",
    "        - global: (global_dim,)\n",
    "        - players: (n_players, player_dim)\n",
    "        - zones: (max_n_zone_vectors, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"global\": torch.from_numpy(current_game_state[\"global\"]).float().to(self.device),\n",
    "            \"players\": torch.from_numpy(current_game_state[\"players\"]).float().to(self.device),\n",
    "            \"zones\": pad_tensor(\n",
    "                torch.from_numpy(current_game_state[\"zones\"]),\n",
    "                pad=self.max_n_zone_vectors,\n",
    "                dim=0,\n",
    "                device=self.device\n",
    "            ).float().to(self.device)\n",
    "        }\n",
    "\n",
    "    def __get_possible_actions_vectors(self, possible_actions: List[Dict[str, np.ndarray]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        possible_actions:\n",
    "        [{\n",
    "          general: (action_dim,)\n",
    "          source_card_vectors: (n_action_source_cards, zone_vector_dim)\n",
    "          target_card_vectors: (n_action_target_cards, zone_vector_dim)\n",
    "        }] * n_possible_actions\n",
    "\n",
    "        Return:\n",
    "\n",
    "        possible_actions_vectors:\n",
    "        - general: (n_possible_actions, action_dim)\n",
    "        - source_card_vectors: (n_possible_actions, max_n_action_source_cards, zone_vector_dim)\n",
    "        - target_card_vectors: (n_possible_actions, max_n_action_target_cards, zone_vector_dim)\n",
    "        \"\"\"\n",
    "        return self.__action_list_to_tensors(action_list=possible_actions)\n",
    "\n",
    "    def __get_target_action(self, n_possible_actions: int, chosen_action_index: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return:\n",
    "\n",
    "        target_action: (n_possible_actions,)\n",
    "        \"\"\"\n",
    "        target_action = torch.zeros(n_possible_actions).float().to(self.device)\n",
    "        target_action[chosen_action_index] = 1\n",
    "        return target_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_learning_dataset = DeepLearningDataset(\n",
    "    player_dataset=dataset_from_pickles,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_possible_actions_collate_fn(\n",
    "    samples: List[Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]],\n",
    "    device\n",
    ") -> Tuple[Dict[str, torch.Tensor], Dict[str, torch.Tensor], Dict[str, torch.Tensor], torch.Tensor]:\n",
    "    batch_action_history_vectors = {}\n",
    "    batch_current_game_state_vectors = {}\n",
    "    batch_possible_actions_vectors = {}\n",
    "    batch_target_action = []\n",
    "    batch_n_possible_actions = []\n",
    "\n",
    "    max_n_possible_actions = max([sample[2][\"general\"].shape[0] for sample in samples])\n",
    "\n",
    "    for sample in samples:\n",
    "        for key, tensor in sample[0].items():\n",
    "            if key not in batch_action_history_vectors:\n",
    "                batch_action_history_vectors[key] = []\n",
    "            batch_action_history_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[1].items():\n",
    "            if key not in batch_current_game_state_vectors:\n",
    "                batch_current_game_state_vectors[key] = []\n",
    "            batch_current_game_state_vectors[key].append(tensor[None])\n",
    "        for key, tensor in sample[2].items():\n",
    "            if key not in batch_possible_actions_vectors:\n",
    "                batch_possible_actions_vectors[key] = []\n",
    "            tensor = pad_tensor(tensor, pad=max_n_possible_actions, dim=0, device=device)\n",
    "            batch_possible_actions_vectors[key].append(tensor[None])\n",
    "\n",
    "        padded_target_action = pad_tensor(\n",
    "            sample[3],\n",
    "            pad=max_n_possible_actions,\n",
    "            dim=0,\n",
    "            device=device\n",
    "        )\n",
    "        batch_target_action.append(padded_target_action[None])\n",
    "        batch_n_possible_actions.append(int(sample[3].shape[0]))\n",
    "\n",
    "    for key, tensors in batch_action_history_vectors.items():\n",
    "        batch_action_history_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    for key, tensors in batch_current_game_state_vectors.items():\n",
    "        batch_current_game_state_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    for key, tensors in batch_possible_actions_vectors.items():\n",
    "        batch_possible_actions_vectors[key] = torch.cat(tensors, dim=0).to(device)\n",
    "    batch_target_action = torch.cat(batch_target_action, dim=0).to(device)\n",
    "    batch_n_possible_actions = torch.from_numpy(np.array(batch_n_possible_actions)).to(device)\n",
    "    \n",
    "    return (\n",
    "        batch_action_history_vectors,\n",
    "        batch_current_game_state_vectors,\n",
    "        batch_possible_actions_vectors,\n",
    "        batch_target_action,\n",
    "        batch_n_possible_actions\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDeepLearningScorer(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "\n",
    "        batch_action_history_vectors:\n",
    "        {\n",
    "            general: (batch_size, history_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, history_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, history_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_current_game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        batch_possible_actions_vectors:\n",
    "        {\n",
    "            general: (batch_size, max_n_possible_actions_in_batch, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_possible_actions_in_batch, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Returns:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __step(self, batch, batch_idx, base_metric_name):\n",
    "        batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors, batch_target_action, batch_n_possible_actions = batch\n",
    "        batch_predicted_target_action = self.forward(\n",
    "            batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors\n",
    "        )\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        for predicted_target_action, n_possible_actions, target_action in zip(batch_predicted_target_action, batch_n_possible_actions, batch_target_action):\n",
    "            batch_loss += self.loss(predicted_target_action[:n_possible_actions], target_action[:n_possible_actions]) / n_possible_actions\n",
    "        batch_loss /= len(batch_predicted_target_action)\n",
    "\n",
    "        self.log(f\"{base_metric_name}_loss\", batch_loss, on_epoch=True, prog_bar=True)\n",
    "        return batch_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"training\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.__step(batch=batch, batch_idx=batch_idx, base_metric_name=\"validation\")\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        batch_game_state_vectors, batch_action_vectors = batch\n",
    "        batch_predicted_scores = self.forward(\n",
    "            batch_game_state_vectors=batch_game_state_vectors, batch_action_vectors=batch_action_vectors\n",
    "        )\n",
    "        return batch_predicted_scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "\n",
    "    def get_n_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionProcessingBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.general_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.action_general_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.card_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, action_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - action_vectors:\n",
    "        {\n",
    "            general: (batch_size, action_general_dim)\n",
    "            source_card_vectors: (batch_size, max_n_action_source_cards, zone_vector_dim)\n",
    "            target_card_vectors: (batch_size, max_n_action_target_cards, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - action_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = action_vectors[\"general\"].shape[0]\n",
    "\n",
    "        action_general_embedding = self.__get_action_general_embedding(action_vectors[\"general\"])\n",
    "        action_source_card_embeddings = self.__get_action_source_card_embeddings(action_vectors[\"source_card_vectors\"])\n",
    "        action_target_card_embeddings = self.__get_action_target_card_embeddings(action_vectors[\"target_card_vectors\"])\n",
    "\n",
    "        action_embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim).to(action_general_embedding)\n",
    "\n",
    "        action_embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                action_embedding_for_prediction,\n",
    "                action_general_embedding[:, None],\n",
    "                action_source_card_embeddings,\n",
    "                action_target_card_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        action_embeddings_sequence_after_transformer = self.transformer_encoder(action_embeddings_sequence)\n",
    "\n",
    "        return action_embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_action_general_embedding(self, action_general_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_general_vector.shape[0]\n",
    "        action_general_embedding = self.general_mlp(action_general_vector)\n",
    "        action_general_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1).to(action_general_embedding)\n",
    "        return torch.cat([action_general_embedding, action_general_embedding_type], dim=1)\n",
    "\n",
    "    def __get_action_source_card_embeddings(self, action_source_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_source_card_vectors.shape[0]\n",
    "        action_source_card_embeddings = self.card_mlp(action_source_card_vectors)\n",
    "        action_source_card_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.max_n_action_source_cards, 1).to(action_source_card_embeddings)\n",
    "        return torch.cat([action_source_card_embeddings, action_source_card_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_action_target_card_embeddings(self, action_target_card_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = action_target_card_vectors.shape[0]\n",
    "        action_target_card_embeddings = self.card_mlp(action_target_card_vectors)\n",
    "        action_target_card_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_action_target_cards, 1).to(action_target_card_embeddings)\n",
    "        return torch.cat([action_target_card_embeddings, action_target_card_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class GameStateProcessingBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        output_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        assert output_dim > 3\n",
    "        self.output_dim = output_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.global_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.game_state_global_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.player_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.player_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.zone_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.zone_vector_dim, out_features=self.output_dim - 3),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.output_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, game_state_vectors: Dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - game_state_vectors:\n",
    "        {\n",
    "            global: (batch_size, game_state_global_dim)\n",
    "            players: (batch_size, n_players, player_dim)\n",
    "            zones: (batch_size, max_n_zone_vectors, zone_vector_dim)\n",
    "        }\n",
    "\n",
    "        Outputs:\n",
    "        - game_state_embedding: (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        batch_size = game_state_vectors[\"global\"].shape[0]\n",
    "\n",
    "        global_embedding = self.__get_global_embedding(game_state_vectors[\"global\"])\n",
    "        player_embeddings = self.__get_player_embeddings(game_state_vectors[\"players\"])\n",
    "        zone_embeddings = self.__get_zone_embeddings(game_state_vectors[\"zones\"])\n",
    "\n",
    "        embedding_for_prediction = torch.zeros(batch_size, 1, self.output_dim).to(global_embedding)\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                embedding_for_prediction,\n",
    "                global_embedding[:, None],\n",
    "                player_embeddings,\n",
    "                zone_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        return embeddings_sequence_after_transformer[:, 0, :]\n",
    "\n",
    "    def __get_global_embedding(self, game_state_global_vector: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_global_vector.shape[0]\n",
    "        global_embedding = self.global_mlp(game_state_global_vector)\n",
    "        global_embedding_type = torch.tensor([[1.0, 0.0, 0.0]]).repeat(batch_size, 1).to(global_embedding)\n",
    "        return torch.cat([global_embedding, global_embedding_type], dim=1)\n",
    "\n",
    "    def __get_player_embeddings(self, game_state_player_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_player_vectors.shape[0]\n",
    "        player_embeddings = self.player_mlp(game_state_player_vectors)\n",
    "        player_embeddings_type = torch.tensor([[[0.0, 1.0, 0.0]]]).repeat(batch_size, self.n_players, 1).to(player_embeddings)\n",
    "        return torch.cat([player_embeddings, player_embeddings_type], dim=2)\n",
    "\n",
    "    def __get_zone_embeddings(self, game_state_zone_vectors: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = game_state_zone_vectors.shape[0]\n",
    "        zone_embeddings = self.zone_mlp(game_state_zone_vectors)\n",
    "        zone_embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, self.max_n_zone_vectors, 1).to(zone_embeddings)\n",
    "        return torch.cat([zone_embeddings, zone_embeddings_type], dim=2)\n",
    "\n",
    "\n",
    "class ClassificationBlock(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        transformer_n_layers: int = 1,\n",
    "        transformer_n_heads: int = 1,\n",
    "        transformer_dim_feedforward: int = 128,\n",
    "        dropout: float = 0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.preprocessing_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_dim + 3, out_features=self.input_dim),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
    "            encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "                d_model=self.input_dim,\n",
    "                nhead=self.transformer_n_heads,\n",
    "                dim_feedforward=self.transformer_dim_feedforward,\n",
    "                dropout=self.dropout,\n",
    "                activation=\"relu\",\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=self.transformer_n_layers\n",
    "        )\n",
    "        self.final_mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_features=self.input_dim, out_features=1)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batch_action_history_embeddings: torch.Tensor,\n",
    "        batch_current_game_state_embedding: torch.Tensor,\n",
    "        batch_possible_actions_embeddings: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - batch_action_history_embeddings: (batch_size, history_size, input_dim)\n",
    "        - batch_current_game_state_embedding: (batch_size, input_dim)\n",
    "        - batch_possible_actions_embeddings: (batch_size, max_n_possible_actions_in_batch, input_dim)\n",
    "\n",
    "        Outputs:\n",
    "        - batch_predicted_target_action: (batch_size, max_n_possible_actions_in_batch)\n",
    "        \"\"\"\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "\n",
    "        batch_action_history_embeddings = self.__prepare_action_history_embeddings(\n",
    "            batch_action_history_embeddings\n",
    "        )\n",
    "        batch_current_game_state_embedding = self.__prepare_current_game_state_embedding(\n",
    "            batch_current_game_state_embedding\n",
    "        )\n",
    "        batch_possible_actions_embeddings = self.__prepare_possible_actions_embeddings(\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        embeddings_sequence = torch.cat(\n",
    "            [\n",
    "                batch_action_history_embeddings,\n",
    "                batch_current_game_state_embedding,\n",
    "                batch_possible_actions_embeddings\n",
    "            ],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        embeddings_sequence_after_transformer = self.transformer_encoder(embeddings_sequence)\n",
    "\n",
    "        possible_actions_embeddings = embeddings_sequence_after_transformer[:, -max_n_possible_actions_in_batch:]\n",
    "\n",
    "        predicted_target_action = self.final_mlp(possible_actions_embeddings)[..., 0]\n",
    "\n",
    "        return predicted_target_action\n",
    "\n",
    "    def __prepare_action_history_embeddings(self, batch_action_history_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_action_history_embeddings.shape[0]\n",
    "        history_size = batch_action_history_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[1.0, 0.0, 0.0]]]).repeat(batch_size, history_size, 1).to(batch_action_history_embeddings)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_action_history_embeddings, embeddings_type], dim=2))\n",
    "\n",
    "    def __prepare_current_game_state_embedding(self, batch_current_game_state_embedding: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_current_game_state_embedding.shape[0]\n",
    "        embedding_type = torch.tensor([[0.0, 1.0, 0.0]]).repeat(batch_size, 1).to(batch_current_game_state_embedding)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_current_game_state_embedding, embedding_type], dim=1)[:, None])\n",
    "\n",
    "    def __prepare_possible_actions_embeddings(self, batch_possible_actions_embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = batch_possible_actions_embeddings.shape[0]\n",
    "        max_n_possible_actions_in_batch = batch_possible_actions_embeddings.shape[1]\n",
    "        embeddings_type = torch.tensor([[[0.0, 0.0, 1.0]]]).repeat(batch_size, max_n_possible_actions_in_batch, 1).to(batch_possible_actions_embeddings)\n",
    "        return self.preprocessing_mlp(torch.cat([batch_possible_actions_embeddings, embeddings_type], dim=2))\n",
    "\n",
    "\n",
    "class DeepLearningScorerV1(BaseDeepLearningScorer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        game_state_global_dim: int,\n",
    "        n_players: int,\n",
    "        player_dim: int,\n",
    "        max_n_zone_vectors: int,\n",
    "        zone_vector_dim: int,\n",
    "        action_general_dim: int,\n",
    "        max_n_action_source_cards: int,\n",
    "        max_n_action_target_cards: int,\n",
    "        embedding_dim: int,\n",
    "        transformer_n_layers: int,\n",
    "        transformer_n_heads: int,\n",
    "        transformer_dim_feedforward: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.game_state_global_dim = game_state_global_dim\n",
    "        self.n_players = n_players\n",
    "        self.player_dim = player_dim\n",
    "        self.max_n_zone_vectors = max_n_zone_vectors\n",
    "        self.zone_vector_dim = zone_vector_dim\n",
    "        self.action_general_dim = action_general_dim\n",
    "        self.max_n_action_source_cards = max_n_action_source_cards\n",
    "        self.max_n_action_target_cards = max_n_action_target_cards\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.transformer_n_layers = transformer_n_layers\n",
    "        self.transformer_n_heads = transformer_n_heads\n",
    "        self.transformer_dim_feedforward = transformer_dim_feedforward\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Modules\n",
    "        self.action_processing_block = ActionProcessingBlock(\n",
    "            action_general_dim=self.action_general_dim,\n",
    "            max_n_action_source_cards=self.max_n_action_source_cards,\n",
    "            max_n_action_target_cards=self.max_n_action_target_cards,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=1,\n",
    "            transformer_dim_feedforward=128,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.game_state_processing_block = GameStateProcessingBlock(\n",
    "            game_state_global_dim=self.game_state_global_dim,\n",
    "            n_players=self.n_players,\n",
    "            player_dim=self.player_dim,\n",
    "            max_n_zone_vectors=self.max_n_zone_vectors,\n",
    "            zone_vector_dim=self.zone_vector_dim,\n",
    "            output_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=self.transformer_n_heads,\n",
    "            transformer_dim_feedforward=self.transformer_dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.classification_block = ClassificationBlock(\n",
    "            input_dim=self.embedding_dim,\n",
    "            transformer_n_layers=self.transformer_n_layers,\n",
    "            transformer_n_heads=self.transformer_n_heads,\n",
    "            transformer_dim_feedforward=self.transformer_dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, batch_action_history_vectors, batch_current_game_state_vectors, batch_possible_actions_vectors):\n",
    "        batch_action_history_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_action_history_vectors,\n",
    "            n_actions=batch_action_history_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_current_game_state_embedding = self.game_state_processing_block(batch_current_game_state_vectors)\n",
    "\n",
    "        batch_possible_actions_embeddings = self.__process_action_list(\n",
    "            batch_action_list_vectors=batch_possible_actions_vectors,\n",
    "            n_actions=batch_possible_actions_vectors[\"general\"].shape[1]\n",
    "        )\n",
    "\n",
    "        batch_predicted_target_action = self.classification_block(\n",
    "            batch_action_history_embeddings,\n",
    "            batch_current_game_state_embedding,\n",
    "            batch_possible_actions_embeddings\n",
    "        )\n",
    "\n",
    "        return batch_predicted_target_action\n",
    "\n",
    "    def __process_action_list(self, batch_action_list_vectors: torch.Tensor, n_actions: int) -> torch.Tensor:\n",
    "        action_embeddings = []\n",
    "        for i in range(n_actions):\n",
    "            one_action_vectors = {key: tensor[:, i] for key, tensor in batch_action_list_vectors.items()}\n",
    "            action_embedding = self.action_processing_block(one_action_vectors)\n",
    "            action_embeddings.append(action_embedding[:, None])\n",
    "        return torch.cat(action_embeddings, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLearningScorerV1(\n",
    "    game_state_global_dim=game_state_global_dim,\n",
    "    n_players=n_players,\n",
    "    player_dim=player_dim,\n",
    "    max_n_zone_vectors=max_n_zone_vectors,\n",
    "    zone_vector_dim=zone_vector_dim,\n",
    "    action_general_dim=action_general_dim,\n",
    "    max_n_action_source_cards=max_n_action_source_cards,\n",
    "    max_n_action_target_cards=max_n_action_target_cards,\n",
    "    embedding_dim=embedding_dim,\n",
    "    transformer_n_layers=transformer_n_layers,\n",
    "    transformer_n_heads=transformer_n_heads,\n",
    "    transformer_dim_feedforward=transformer_dim_feedforward,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, validation_dataset = torch.utils.data.random_split(\n",
    "    deep_learning_dataset,\n",
    "    lengths=[\n",
    "        0.8,\n",
    "        0.2,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_loader = torch.utils.data.DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda samples: pad_possible_actions_collate_fn(samples=samples, device=device)\n",
    ")\n",
    "validation_data_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=10,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda samples: pad_possible_actions_collate_fn(samples=samples, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder_path = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/gcoter/projects/personal/magic-the-gathering-python/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n"
     ]
    }
   ],
   "source": [
    "Path(model_folder_path).mkdir(parents=True, exist_ok=True)\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        dirpath=model_folder_path,\n",
    "        filename=\"deep_learning_scorer\",\n",
    "        monitor=\"validation_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        verbose=False\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor=\"validation_loss\",\n",
    "        mode=\"min\",\n",
    "        patience=10,\n",
    "        verbose=False\n",
    "    ),\n",
    "]\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    devices=\"auto\",\n",
    "    deterministic=True,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/06/25 11:20:23 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/gcoter/projects/personal/magic-the-gathering-python/venv/lib/python3.10/site-packages/mlflow/pytorch/_lightning_autolog.py:351: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.0.5 and 2.0.2 and may not succeed with packages outside this range.\"\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/gcoter/projects/personal/magic-the-gathering-python/venv/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /home/gcoter/projects/personal/magic-the-gathering-python/notebooks/results exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name                        | Type                     | Params\n",
      "-------------------------------------------------------------------------\n",
      "0 | loss                        | CrossEntropyLoss         | 0     \n",
      "1 | action_processing_block     | ActionProcessingBlock    | 37.6 K\n",
      "2 | game_state_processing_block | GameStateProcessingBlock | 36.3 K\n",
      "3 | classification_block        | ClassificationBlock      | 37.9 K\n",
      "-------------------------------------------------------------------------\n",
      "111 K     Trainable params\n",
      "0         Non-trainable params\n",
      "111 K     Total params\n",
      "0.447     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9b11362b904f06b3b185d59ae5b211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcoter/projects/personal/magic-the-gathering-python/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/gcoter/projects/personal/magic-the-gathering-python/venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca283d91eeb7428d8588f518afc6ad10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baca0c8432b4ef2a8e58797a3f4a859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5280fa00e56c4dc38dd160474fa42745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3ebcbf6e9e440e8602ab5199ce3584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "970b3b4dfc1842aea1f44cdb3d0dd78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207a1eb4f9194530aca5e7b00652e2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c86af6f385240fe8556204fb22702fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d76256d39374184b64b79ac55e26a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75aad056be9b480fac1edb553554b86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c5eb5184fb4744a6564999a4ab4752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4c176d50644a48844779dae8544e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77251b71978d4412aafe5277b0e7190e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1021922fda54f9ca9a68431684e1638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4911e9f3ed8048da90edefac0922cc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39abf287abf4e0daa9140dec8303b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bddce7a064f41148d170e23342c5e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810b28649d184e7394300e00fff607c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9e528cc6404c1cb7b63d48bd65a7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca94608316db4eae9c30cdeaee8e8256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255c2bcc19e94c8bbbd8fa2a72fab411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee47fa1c5702467f97e2139939d8b24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=training_data_loader,\n",
    "    val_dataloaders=validation_data_loader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
